# Enhancing-Intent-Detection-in-NRT-Tweets-Using-LLM

![Intent Detection Banner](PLOT01.png)


## ğŸ§  Overview

Although smoking cessation aids like Nicotine Replacement Therapy (NRT) are available, quit rates remain low â€” in part due to the improper use of these aids. Online support groups can increase accessibility, but delivering consistent and timely responses is labor-intensive.

This project explores the development of an AI-powered intent detection system for NRT-related online communities using Large Language Models (LLMs). The goal is to accurately identify user intentions to provide responsive, context-aware support.

---

## ğŸ¯ Objective

To build a responsive and attentive AI listener that can:

- Understand nuanced and diverse user intents in online NRT support forums.
- Address the challenges of **data imbalance** and **complexity in multiple intent classes**.
- Deliver **timely and appropriate** support responses by detecting user intent with high accuracy.

---

## ğŸ› ï¸ Methods

The proposed methodology consists of **four progressive stages**:

1. **Zero-Shot Inference:**
   - Use a pre-trained LLM without fine-tuning on the dataset.
   
2. **Domain-Specific Fine-Tuning:**
   - Fine-tune the LLM on a nicotine replacement therapy (NRT) dataset.
   
3. **Class Imbalance Handling:**
   - Apply **downsampling** to the majority class and re-fine-tune the model.
   
4. **Error Correction + Fine-Tuning:**
   - Identify and fix misclassified samples.
   - Re-fine-tune the model on the corrected, downsampled dataset.

Each step addresses limitations of the previous one and contributes to incremental improvements in classification performance.

---

## ğŸ“Š Results

By combining:

- **Downsampling of the majority class**
- **Domain-specific fine-tuning**
- **Targeted error correction**

The final model achieved:

- **Unweighted F1-Score:** 86%
- **Weighted F1-Score:** 90%

These results represent:

- ğŸ“ˆ **28% improvement** in unweighted F1-Score
- ğŸ“ˆ **7% improvement** in weighted F1-Score

compared to earlier setups.

---

## ğŸ§¾ Conclusion

- LLMs **without domain-specific fine-tuning** perform poorly on intent detection in the NRT context.
- **Data imbalance** negatively affects performance, even with fine-tuning.
- **Downsampling** helps mitigate this issue but may introduce noise.
- **Manual correction of misclassified examples**, followed by fine-tuning, leads to **significant gains** in performance.

---

## ğŸ’¡ Key Takeaways

- Accurate intent detection is **crucial** for responsive support in smoking cessation communities.
- A **stepwise refinement** approach leveraging LLMs can effectively handle class imbalance and language complexity.
- Combining **machine learning with human insight** (through error correction) produces the best results.

---

## ğŸ“ Repository Structure

```bash

Enhancing-Intent-Detection-in-NRT-Tweets-Using-LLM
â”œâ”€â”€ ğŸ“ all_Data+LLM Finetune
â”‚ â”œâ”€â”€ Final_Data_Processing-all-data.ipynb â€“ Preprocessing on all available data
â”‚ â”œâ”€â”€ finetune.py â€“ Fine-tuning the LLM on the full dataset
â”‚ â””â”€â”€ output_log.log â€“ Log file for model training and evaluation
â”‚
â”œâ”€â”€ ğŸ“ data_downsample + error correction + LLM Finetune
â”‚ â”œâ”€â”€ finetune.py â€“ Fine-tuning after downsampling and correcting label errors
â”‚ â””â”€â”€ output_train.log â€“ Log for final fine-tuning stage
â”‚
â”œâ”€â”€ ğŸ“ data_downsample_version+ LLM Finetune
â”‚ â”œâ”€â”€ Final_Data_Processing.ipynb â€“ Preprocessing with downsampled data
â”‚ â”œâ”€â”€ finetune.py â€“ Fine-tuning on downsampled data
â”‚ â””â”€â”€ output_log.log â€“ Log of training
â”‚
â””â”€â”€ ğŸ“ zeroShot
â”œâ”€â”€ Final_Data_Processing.ipynb â€“ Input preparation for zero-shot predictions
â”œâ”€â”€ predictions_by_llm.py â€“ Zero-shot predictions using base LLM
â””â”€â”€ Scores.ipynb â€“ Evaluation and score computation

```

---

### ğŸ§¾ Explanation

- **`all_Data+LLM Finetune/`**: This folder contains scripts and logs for fine-tuning the LLM on the complete dataset without handling imbalance.
- **`data_downsample_version+ LLM Finetune/`**: This folder introduces downsampling of the majority class before training the LLM.
- **`data_downsample + error correction + LLM Finetune/`**: Final stage where data was cleaned, errors corrected, and then used for fine-tuning. This achieved the best results.
- **`zeroShot/`**: Used to test the zero-shot performance of a pre-trained LLM without any fine-tuning, providing a baseline for comparison.

---
